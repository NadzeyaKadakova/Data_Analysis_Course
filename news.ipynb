{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Работа со встроенным датасетом fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news=fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изучаю данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет представляет собой словарь (совокупность нескольких \"пакетов\" данных)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR', 'description'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В спеке на scikit описано, что датасет разделен на тренировочное и тестовое подмножество. Как я выяснила, по умолчанию подгружается тренировочное подмножество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check2=fetch_20newsgroups(subset='all')\n",
    "len(check2.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check1=fetch_20newsgroups(subset='train')\n",
    "len(check1.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(news.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет <b>news</b> по умолчанию содержит подмножества <b>DESCR</b> (NoneType), <b>data</b> (list), <b>description</b> (str), <b>description</b> (numpy.ndarray), <b>targat</b> (numpy.ndarray), <b>target_names</b> (list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В датасете есть следующие категории писем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.target_names  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "проверила, что их 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Письма содержатся в подмножестве data. Далее я для примера вывела одно письмо. Изначально они представлены в неудобном для чтения виде, поэтому я разделила письмо по символу \\n с помощью функции split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: mathew <mathew@mantis.co.uk>\n",
      "Subject: Re: <Political Atheists?\n",
      "Organization: Mantis Consultants, Cambridge. UK.\n",
      "X-Newsreader: rusnews v1.01\n",
      "Lines: 22\n",
      "\n",
      "kmr4@po.CWRU.edu (Keith M. Ryan) writes:\n",
      "> ( I am almost sure that Zyklon-B is immediate and painless method of \n",
      "> death. If not, insert soem other form. )\n",
      "> \n",
      ">         And, ethnic and minority groups have been killed, mutilated and \n",
      "> exterminated through out history, so I guess it was not unusual.\n",
      "> \n",
      ">         So, you would agree that the holocost would be allowed under the US \n",
      "> Constitution?  [ in so far, the punishment. I doubt they recieved what would \n",
      "> be considered a \"fair\" trial by US standards.\n",
      "\n",
      "Don't be so sure.  Look what happened to Japanese citizens in the US during\n",
      "World War II.  If you're prepared to say \"Let's round these people up and\n",
      "stick them in a concentration camp without trial\", it's only a short step to\n",
      "gassing them without trial.  After all, it seems that the Nazis originally\n",
      "only intended to imprison the Jews; the Final Solution was dreamt up partly\n",
      "because they couldn't afford to run the camps because of the devastation\n",
      "caused by Goering's Total War.  Those who weren't gassed generally died of\n",
      "malnutrition or disease.\n",
      "\n",
      "\n",
      "mathew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in news.data[15].split('\\n'):\n",
    "        print(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всего в подмножестве news.data содержится 11314 писем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства работы, я создала датафрейм на основе датасета. (это вовсе не обязательно, просто я решила так сделать)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ddata=pd.DataFrame(data=news.data,columns=['Message'])\n",
    "ddata['target']=news.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>From: dfo@vttoulu.tko.vtt.fi (Foxvog Douglas)\\...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>From: bmdelane@quads.uchicago.edu (brian manni...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>From: bgrubb@dante.nmsu.edu (GRUBB)\\nSubject: ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>From: holmes7000@iscsvax.uni.edu\\nSubject: WIn...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>From: kerr@ux1.cso.uiuc.edu (Stan Kerr)\\nSubje...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Message  target\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7\n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4\n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4\n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1\n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14\n",
       "5  From: dfo@vttoulu.tko.vtt.fi (Foxvog Douglas)\\...      16\n",
       "6  From: bmdelane@quads.uchicago.edu (brian manni...      13\n",
       "7  From: bgrubb@dante.nmsu.edu (GRUBB)\\nSubject: ...       3\n",
       "8  From: holmes7000@iscsvax.uni.edu\\nSubject: WIn...       2\n",
       "9  From: kerr@ux1.cso.uiuc.edu (Stan Kerr)\\nSubje...       4"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddata.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По заданию, мне нужно было выбрать 4 категории и классифицировать письма согласно этим категориям.\n",
    "Я выбрала Автомобили, Медицину, Космос и Религию (Христианство)\n",
    "Когда я это сделала, в первой колонке остались старые индексы. Поэтому я применила функцию reset_index, чтобы индексы установить заново\n",
    "Получилась таблица, которая соотносит оставшиеся 2380 писем с их таргетами (номерами категорий)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2380, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: bmdelane@quads.uchicago.edu (brian manni...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: dbm0000@tm0006.lerc.nasa.gov (David B. M...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: CPKJP@vm.cc.latech.edu (Kevin Parker)\\nS...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>From: jonh@david.wheaton.edu (Jonathan Hayward...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>From: jimf@centerline.com (Jim Frost)\\nSubject...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Subject: Teenage acne\\nFrom: pchurch@swell.act...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>From: 18084TM@msu.edu (Tom)\\nSubject: Golden &amp;...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>From: dlecoint@garnet.acns.fsu.edu (Darius_Lec...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Message  target\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7\n",
       "1  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14\n",
       "2  From: bmdelane@quads.uchicago.edu (brian manni...      13\n",
       "3  From: dbm0000@tm0006.lerc.nasa.gov (David B. M...      14\n",
       "4  From: CPKJP@vm.cc.latech.edu (Kevin Parker)\\nS...       7\n",
       "5  From: jonh@david.wheaton.edu (Jonathan Hayward...      15\n",
       "6  From: jimf@centerline.com (Jim Frost)\\nSubject...       7\n",
       "7  Subject: Teenage acne\\nFrom: pchurch@swell.act...      13\n",
       "8  From: 18084TM@msu.edu (Tom)\\nSubject: Golden &...      14\n",
       "9  From: dlecoint@garnet.acns.fsu.edu (Darius_Lec...      15"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catdata=ddata.loc[ddata['target'].isin([7, 13, 14, 15])]\n",
    "catdata=catdata.reset_index(drop=True)\n",
    "print(catdata.shape)\n",
    "catdata[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Далее я изучала основные понятия, которые используют при анализе текстовых данных.\n",
    "\n",
    "#### n-граммы\n",
    "Прежде всего я создавала n-граммы, которые явялются одним из основных понятий в анализе текстовых данных. \n",
    "n-граммы, по сути, являются последовательностями сабстрингов длины n исходного текста. \n",
    "Например, если у нас есть текст, состоящий из 5 слов (от 1 до 5), то n-граммой \"по словам\" при n=3 , будет являтся последовательность слов (1,2,3), (2,3,4), (3,4,5). Также есть n-граммы по символам. Алгоритм для них такой же, за исключением того, что в данном случае можно выделить 2 вида  n-грамм: те, которые работают только с начала и до конца слова, и те, которые игнорируют пробелы, и работают на всей последовательности символов текста.\n",
    "\n",
    "Для создания n-грамм я взяла CountVectorizer. Он принимает на вход массив текстовых данных и возвращает вектор признаков (набор чисел). То есть я  представила признаки в числовом формате, так как классификатор работает с числовыми данными. \n",
    "\n",
    "Далеее я для примера преобразовала 16-е по счету письмо в тренировочной выборке в набор чисел. \n",
    "По сути, чем больше число в получившейся матрице, тем большие вес, id est большую уникальность имеет данное слово."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 3, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 2, 3, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2,\n",
       "         3, 9, 2, 1, 2, 1, 1, 5, 1, 3, 2, 1, 1, 2, 3, 1, 2, 2, 1, 2, 1, 2,\n",
       "         1, 3, 1, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect=CountVectorizer( ngram_range=(1,1),analyzer='word')\n",
    "matr=vect.fit_transform([news.data[15]])\n",
    "matr.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее, я использовала tf-idf.\n",
    "\n",
    "tf-idf применяется, когда у нас имеется набор (коллекция) текстов, в данном случае - писем. tf-idf оценивает важность конкретного слово во всем наборе писем. Вес слова пропорционален количеству употреблений слова в письме, и обратно пропорционален количеству употреблений данного слова в остальных писмах нашего набора.\n",
    "\n",
    "Рассчитывается: term frequency * inverted document frequency\n",
    "\n",
    "Например:\n",
    "\n",
    "Возьмем текст А, посчитаем количество вхождений слова 'nadz' в этот текст.\n",
    "Пусть слов в тексте у нас 1000, а the  встречается 10 раз.\n",
    "\n",
    "tf('nadz')=0.01\n",
    "\n",
    "Чтобы посчитать inverted document frequency для этого слова, нужно взять логарифм от отношения количества текстов, в которых есть хоть одно вхождение этого слова, к общему количеству текстов. Думаю, логарифм берут для нормализации данных, чтобы не было большого разброса.\n",
    "\n",
    "Выбор основания логарифма в формуле не имеет значения, поскольку изменение основания приводит к изменению веса каждого слова на постоянный множитель, что не влияет на соотношение весов.\n",
    "\n",
    "Если всего текстов 10000, и слово “nadz” встретилось в 100 текстах, то\n",
    "\n",
    "idf(“nadz”) = log(10000/100) = 4.6 \n",
    "\n",
    "Таким образом, частые слова имеют минимальный вес, и специфичные редкие — большой\n",
    "\n",
    "в Python для вычисления tf-idf используется TfidfTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "X=catdata['Message']\n",
    "y=catdata['target']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее, создаем пайплайн, который по сути определет последовательность действий, которые будут выполнены.\n",
    "В нашем случае, сначала мы сначала применяем к набору писем CountVectorizer, который преобразует каждое письмо в матрицу чисел, в соответствии с весом (уникальностью) каждого слова в конкретном письме. При этом, мы установили параметр stop_words - частовстречающихся слов, которые будут игнорироваться при анализе. Мы используем встроенный стоп-лист слов для англоязычных текстов 'english'.\n",
    "\n",
    "При подборе параметров, я использовала разные функции для анализа - word, char, char_wb, word, которые отвечают за то, из чего будет стоиться n-грамма - из слов, или из символов (с учетом разделения слов пробелами, или без).\n",
    "\n",
    "Наилучший результат получился, при построении n-грамм по словам. Это и логично, ведь тексты по тематике можно отличить прежде всего ключевыми словами.\n",
    "\n",
    "Итак, на данном этапе мы получили матрицу, состоящую из чисел.\n",
    "\n",
    "Далее, нам нужно нормализовать получившуюся матрицу. Для этого используем TfidfTransformer.\n",
    "\n",
    "Норму я выбрала L2. Эта норма соответствует Евклидовой норме, которая вычисляется в случае матрицы как квадратный корень суммы квадратов каждого элемента.\n",
    "\n",
    "На данном этапе я получила нормализованную матрицу.\n",
    "\n",
    "Далее, я построила классификатор стохастического градиентного спуска (SGD).\n",
    "penalty отвечает за функцию регуляризации, я выбрала L2, которая является Ridge регуляризацией.\n",
    "В качестве loss функции используем Huber loss, которая менее чувствительная к выбросам (штрафует их поменьше). Т.е, сначала она работает как  squared error loss, а потом, начиная с определенного большого значения работает линейно. Это позволяет уменьшить \"разброс\"\n",
    "\n",
    "А для подбора наиболее подходящих параметров я использовала RandomizedSearchCV. GreadSearch не взяла, потому что он использует максимальный score (может сильно отличаться от минимального, и в результате среднее значение будет хуже). А RandomizedSearch выбирает среднее значение. \n",
    "Значений перебрала много, самые оптимальные использовала дальше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9802875559338159 +/- 0.009582259367351628\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(stop_words = 'english',analyzer='word')),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    ('clf', SGDClassifier(penalty='l2',loss='modified_huber')),\n",
    "    ])\n",
    "param_grid={\n",
    "    'vect__ngram_range': ((1, 6),(1, 7), (3, 7),(3,6)), \n",
    "    'clf__alpha': (0.0000001,0.000001,0.00001),\n",
    "    'vect__max_df':(0.3, 0.5)\n",
    "}\n",
    "search=RandomizedSearchCV(pipe, param_distributions=param_grid,n_iter=20)\n",
    "search.fit(X, y)\n",
    "score = cross_val_score(pipe, X, y, scoring='accuracy', cv=17, n_jobs=-1)\n",
    "print(\"{} +/- {}\".format(score.mean(), score.std())) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функцию report использую для представления результатов. скопировала ее из спеки RandomizedSearch. Она позволяет отсортировать топ-n результатов, а также, соответственно, я вижу параметры которые \"сработали\" лучше, это я использовала при дальнейшем изменении параметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report(results, n_top=5):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.963 (std: 0.006)\n",
      "Parameters: {'vect__ngram_range': (1, 6), 'vect__max_df': 0.5, 'clf__alpha': 1e-05}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.959 (std: 0.008)\n",
      "Parameters: {'vect__ngram_range': (1, 7), 'vect__max_df': 0.3, 'clf__alpha': 1e-05}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.959 (std: 0.004)\n",
      "Parameters: {'vect__ngram_range': (1, 7), 'vect__max_df': 0.5, 'clf__alpha': 1e-05}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 0.958 (std: 0.007)\n",
      "Parameters: {'vect__ngram_range': (1, 6), 'vect__max_df': 0.3, 'clf__alpha': 1e-06}\n",
      "\n",
      "Model with rank: 4\n",
      "Mean validation score: 0.958 (std: 0.005)\n",
      "Parameters: {'vect__ngram_range': (1, 6), 'vect__max_df': 0.3, 'clf__alpha': 1e-07}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report(search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже мне захотелось проверить работу моей модели на двух рандомных текстиках, найденных в гугле. Модель сработала верно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci.space\n",
      "sci.med\n"
     ]
    }
   ],
   "source": [
    "a='To learn more about this FRB, scientists used the Arecibo Observatory in Puerto Rico and the Green Bank Telescope in West Virginia to analyze data on 16 bursts from object. FRB 121102 is located in a star-forming region of a dwarf galaxy found about 3 billion light-years from Earth, Hessels said. Because astronomers can see it from such a great distance, the amount of energy in a single millisecond of each of these bursts must be about as much as the sun releases in an entire day, Hessels and his colleagues said in a statement.'\n",
    "\n",
    "print(news.target_names[search.predict([a])[0]])\n",
    "\n",
    "b='In this phase 3, double-blind trial, which was conducted at 133 centers worldwide, we randomly assigned 1402 patients with type 1 diabetes who were receiving treatment with any insulin therapy (pump or injections) to receive sotagliflozin (400 mg per day) or placebo for 24 weeks. The primary end point was a glycated hemoglobin level lower than 7.0% at week 24, with no episodes of severe hypoglycemia or diabetic ketoacidosis after randomization. Secondary end points included the change from baseline in glycated hemoglobin level, weight, systolic blood pressure, and mean daily bolus dose of insulin'\n",
    "\n",
    "print(news.target_names[search.predict([b])[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тест"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут я протестировала модель на тестовом подмножестве. В результате точность предсказания составила 94 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news2=fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR', 'description'])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ddatatest=pd.DataFrame(data=news2.data,columns=['Message'])\n",
    "ddatatest['target']=news2.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1584, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Message</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: v064mb9k@ubvmsd.cc.buffalo.edu (NEIL B. ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: banschbach@vms.ocom.okstate.edu\\nSubject...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: PETCH@gvg47.gvg.tek.com (Chuck)\\nSubject...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: fortmann@superbowl.und.ac.za (Paul Fortm...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: prb@access.digex.net (Pat)\\nSubject: Re:...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>From: george@crayola.East.Sun.COM (George A. P...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>From: cadiz@rtsg.mot.com (Jay Cadiz)\\nSubject:...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>From: rmakarem@usc.edu (Total Stranger)\\nSubje...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>From: baalke@kelvin.jpl.nasa.gov (Ron Baalke)\\...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>From: abdkw@stdvax.gsfc.nasa.gov (David Ward)\\...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Message  target\n",
       "0  From: v064mb9k@ubvmsd.cc.buffalo.edu (NEIL B. ...       7\n",
       "1  From: banschbach@vms.ocom.okstate.edu\\nSubject...      13\n",
       "2  From: PETCH@gvg47.gvg.tek.com (Chuck)\\nSubject...      15\n",
       "3  From: fortmann@superbowl.und.ac.za (Paul Fortm...      15\n",
       "4  From: prb@access.digex.net (Pat)\\nSubject: Re:...      14\n",
       "5  From: george@crayola.East.Sun.COM (George A. P...      13\n",
       "6  From: cadiz@rtsg.mot.com (Jay Cadiz)\\nSubject:...       7\n",
       "7  From: rmakarem@usc.edu (Total Stranger)\\nSubje...       7\n",
       "8  From: baalke@kelvin.jpl.nasa.gov (Ron Baalke)\\...      14\n",
       "9  From: abdkw@stdvax.gsfc.nasa.gov (David Ward)\\...      14"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata=ddatatest.loc[ddatatest['target'].isin([7, 13, 14, 15])]\n",
    "testdata=testdata.reset_index(drop=True)\n",
    "print(testdata.shape)\n",
    "testdata[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 13, 15, 15, 14, 13,  7,  7, 14, 14])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testpred=p.predict(testdata.Message)\n",
    "testpred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94696969696969702"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(testdata['target'], testpred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
